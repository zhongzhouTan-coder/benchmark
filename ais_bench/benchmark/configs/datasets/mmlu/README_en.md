# MMLU
[ä¸­æ–‡](README.md) | English
## Dataset Introduction
MMLU (Massive Multitask Language Understanding) is a new benchmark designed to measure the world knowledge that large models acquire during pre-training under zero-shot and few-shot scenarios. This makes the benchmark more challenging and more similar to how we evaluate humans. It covers 57 subjects across fields such as STEM, humanities, and social sciences. The difficulty level ranges from elementary to advanced, testing both world knowledge and problem-solving abilities. The subjects span traditional areas like mathematics and history to more specialized fields such as law and ethics. The granularity and breadth of the subjects make this benchmark an ideal choice for identifying the blind spots of models.

> ðŸ”— Dataset Homepage: [https://github.com/hendrycks/test](https://github.com/hendrycks/test)

## Dataset Deployment
- The dataset compressed package can be downloaded from the link provided by OpenCompass ðŸ”—: [http://opencompass.oss-cn-shanghai.aliyuncs.com/datasets/data/mmlu.zip](http://opencompass.oss-cn-shanghai.aliyuncs.com/datasets/data/mmlu.zip).
- It is recommended to deploy the dataset in the directory `{tool_root_path}/ais_bench/datasets` (the default path set for dataset tasks). Taking deployment on a Linux server as an example, the specific execution steps are as follows:
```bash
# Within the Linux server, under the tool root path
cd ais_bench/datasets
wget http://opencompass.oss-cn-shanghai.aliyuncs.com/datasets/data/mmlu.zip
unzip mmlu.zip
rm mmlu.zip
```
- Execute `tree mmlu/` in the directory `{tool_root_path}/ais_bench/datasets` to check the directory structure. If the directory structure matches the one shown below, the dataset has been deployed successfully:
    ```
    mmlu/
    â”œâ”€â”€ dev
    â”‚   â”œâ”€â”€ abstract_algebra_dev.csv
    â”‚   â”œâ”€â”€ anatomy_dev.csv
    â”‚   â”œâ”€â”€ astronomy_dev.csv
    â”‚   â”œâ”€â”€ business_ethics_dev.csv
    â”‚   â”œâ”€â”€ clinical_knowledge_dev.csv
    â”‚   â”œâ”€â”€ college_biology_dev.csv
    â”‚   â”œâ”€â”€ college_chemistry_dev.csv
    â”‚   â”œâ”€â”€ college_computer_science_dev.csv
    â”‚   â”œâ”€â”€ college_mathematics_dev.csv
    â”‚   â”œâ”€â”€ college_medicine_dev.csv
    â”‚   â”œâ”€â”€ college_physics_dev.csv
    â”‚   â”œâ”€â”€ computer_security_dev.csv
    â”‚   â”œâ”€â”€ conceptual_physics_dev.csv
    â”‚   â”œâ”€â”€ econometrics_dev.csv
    â”‚   â”œâ”€â”€ electrical_engineering_dev.csv
    â”‚   â”œâ”€â”€ elementary_mathematics_dev.csv
    â”‚   â”œâ”€â”€ formal_logic_dev.csv
    â”‚   â”œâ”€â”€ global_facts_dev.csv
    â”‚   â”œâ”€â”€ high_school_biology_dev.csv
    â”‚   â”œâ”€â”€ high_school_chemistry_dev.csv
    â”‚   â”œâ”€â”€ high_school_computer_science_dev.csv
    â”‚   â”œâ”€â”€ high_school_european_history_dev.csv
    â”‚   â”œâ”€â”€ high_school_geography_dev.csv
    â”‚   â”œâ”€â”€ high_school_government_and_politics_dev.csv
    â”‚   â”œâ”€â”€ high_school_macroeconomics_dev.csv
    â”‚   â”œâ”€â”€ high_school_mathematics_dev.csv
    â”‚   â”œâ”€â”€ high_school_microeconomics_dev.csv
    â”‚   â”œâ”€â”€ high_school_physics_dev.csv
    â”‚   â”œâ”€â”€ high_school_psychology_dev.csv
    â”‚   â”œâ”€â”€ high_school_statistics_dev.csv
    â”‚   â”œâ”€â”€ high_school_us_history_dev.csv
    â”‚   â”œâ”€â”€ high_school_world_history_dev.csv
    â”‚   â”œâ”€â”€ human_aging_dev.csv
    â”‚   â”œâ”€â”€ human_sexuality_dev.csv
    â”‚   â”œâ”€â”€ international_law_dev.csv
    â”‚   â”œâ”€â”€ jurisprudence_dev.csv
    â”‚   â”œâ”€â”€ logical_fallacies_dev.csv
    â”‚   â”œâ”€â”€ machine_learning_dev.csv
    â”‚   â”œâ”€â”€ management_dev.csv
    â”‚   â”œâ”€â”€ marketing_dev.csv
    â”‚   â”œâ”€â”€ medical_genetics_dev.csv
    â”‚   â”œâ”€â”€ miscellaneous_dev.csv
    â”‚   â”œâ”€â”€ moral_disputes_dev.csv
    â”‚   â”œâ”€â”€ moral_scenarios_dev.csv
    â”‚   â”œâ”€â”€ nutrition_dev.csv
    â”‚   â”œâ”€â”€ philosophy_dev.csv
    â”‚   â”œâ”€â”€ prehistory_dev.csv
    â”‚   â”œâ”€â”€ professional_accounting_dev.csv
    â”‚   â”œâ”€â”€ professional_law_dev.csv
    â”‚   â”œâ”€â”€ professional_medicine_dev.csv
    â”‚   â”œâ”€â”€ professional_psychology_dev.csv
    â”‚   â”œâ”€â”€ public_relations_dev.csv
    â”‚   â”œâ”€â”€ security_studies_dev.csv
    â”‚   â”œâ”€â”€ sociology_dev.csv
    â”‚   â”œâ”€â”€ us_foreign_policy_dev.csv
    â”‚   â”œâ”€â”€ virology_dev.csv
    â”‚   â””â”€â”€ world_religions_dev.csv
    â”œâ”€â”€ possibly_contaminated_urls.txt
    â”œâ”€â”€ README.txt
    â”œâ”€â”€ test
    â”‚   â”œâ”€â”€ abstract_algebra_test.csv
    â”‚   â”œâ”€â”€ anatomy_test.csv
    â”‚   â”œâ”€â”€ astronomy_test.csv
    â”‚   â”œâ”€â”€ business_ethics_test.csv
    â”‚   â”œâ”€â”€ clinical_knowledge_test.csv
    â”‚   â”œâ”€â”€ college_biology_test.csv
    â”‚   â”œâ”€â”€ college_chemistry_test.csv
    â”‚   â”œâ”€â”€ college_computer_science_test.csv
    â”‚   â”œâ”€â”€ college_mathematics_test.csv
    â”‚   â”œâ”€â”€ college_medicine_test.csv
    â”‚   â”œâ”€â”€ college_physics_test.csv
    â”‚   â”œâ”€â”€ computer_security_test.csv
    â”‚   â”œâ”€â”€ conceptual_physics_test.csv
    â”‚   â”œâ”€â”€ econometrics_test.csv
    â”‚   â”œâ”€â”€ electrical_engineering_test.csv
    â”‚   â”œâ”€â”€ elementary_mathematics_test.csv
    â”‚   â”œâ”€â”€ formal_logic_test.csv
    â”‚   â”œâ”€â”€ global_facts_test.csv
    â”‚   â”œâ”€â”€ high_school_biology_test.csv
    â”‚   â”œâ”€â”€ high_school_chemistry_test.csv
    â”‚   â”œâ”€â”€ high_school_computer_science_test.csv
    â”‚   â”œâ”€â”€ high_school_european_history_test.csv
    â”‚   â”œâ”€â”€ high_school_geography_test.csv
    â”‚   â”œâ”€â”€ high_school_government_and_politics_test.csv
    â”‚   â”œâ”€â”€ high_school_macroeconomics_test.csv
    â”‚   â”œâ”€â”€ high_school_mathematics_test.csv
    â”‚   â”œâ”€â”€ high_school_microeconomics_test.csv
    â”‚   â”œâ”€â”€ high_school_physics_test.csv
    â”‚   â”œâ”€â”€ high_school_psychology_test.csv
    â”‚   â”œâ”€â”€ high_school_statistics_test.csv
    â”‚   â”œâ”€â”€ high_school_us_history_test.csv
    â”‚   â”œâ”€â”€ high_school_world_history_test.csv
    â”‚   â”œâ”€â”€ human_aging_test.csv
    â”‚   â”œâ”€â”€ human_sexuality_test.csv
    â”‚   â”œâ”€â”€ international_law_test.csv
    â”‚   â”œâ”€â”€ jurisprudence_test.csv
    â”‚   â”œâ”€â”€ logical_fallacies_test.csv
    â”‚   â”œâ”€â”€ machine_learning_test.csv
    â”‚   â”œâ”€â”€ management_test.csv
    â”‚   â”œâ”€â”€ marketing_test.csv
    â”‚   â”œâ”€â”€ medical_genetics_test.csv
    â”‚   â”œâ”€â”€ miscellaneous_test.csv
    â”‚   â”œâ”€â”€ MMLU_test_contamination_annotations.json
    â”‚   â”œâ”€â”€ moral_disputes_test.csv
    â”‚   â”œâ”€â”€ moral_scenarios_test.csv
    â”‚   â”œâ”€â”€ nutrition_test.csv
    â”‚   â”œâ”€â”€ philosophy_test.csv
    â”‚   â”œâ”€â”€ prehistory_test.csv
    â”‚   â”œâ”€â”€ professional_accounting_test.csv
    â”‚   â”œâ”€â”€ professional_law_test.csv
    â”‚   â”œâ”€â”€ professional_medicine_test.csv
    â”‚   â”œâ”€â”€ professional_psychology_test.csv
    â”‚   â”œâ”€â”€ public_relations_test.csv
    â”‚   â”œâ”€â”€ security_studies_test.csv
    â”‚   â”œâ”€â”€ sociology_test.csv
    â”‚   â”œâ”€â”€ us_foreign_policy_test.csv
    â”‚   â”œâ”€â”€ virology_test.csv
    â”‚   â””â”€â”€ world_religions_test.csv
    â””â”€â”€ val
        â”œâ”€â”€ abstract_algebra_val.csv
        â”œâ”€â”€ anatomy_val.csv
        â”œâ”€â”€ astronomy_val.csv
        â”œâ”€â”€ business_ethics_val.csv
        â”œâ”€â”€ clinical_knowledge_val.csv
        â”œâ”€â”€ college_biology_val.csv
        â”œâ”€â”€ college_chemistry_val.csv
        â”œâ”€â”€ college_computer_science_val.csv
        â”œâ”€â”€ college_mathematics_val.csv
        â”œâ”€â”€ college_medicine_val.csv
        â”œâ”€â”€ college_physics_val.csv
        â”œâ”€â”€ computer_security_val.csv
        â”œâ”€â”€ conceptual_physics_val.csv
        â”œâ”€â”€ econometrics_val.csv
        â”œâ”€â”€ electrical_engineering_val.csv
        â”œâ”€â”€ elementary_mathematics_val.csv
        â”œâ”€â”€ formal_logic_val.csv
        â”œâ”€â”€ global_facts_val.csv
        â”œâ”€â”€ high_school_biology_val.csv
        â”œâ”€â”€ high_school_chemistry_val.csv
        â”œâ”€â”€ high_school_computer_science_val.csv
        â”œâ”€â”€ high_school_european_history_val.csv
        â”œâ”€â”€ high_school_geography_val.csv
        â”œâ”€â”€ high_school_government_and_politics_val.csv
        â”œâ”€â”€ high_school_macroeconomics_val.csv
        â”œâ”€â”€ high_school_mathematics_val.csv
        â”œâ”€â”€ high_school_microeconomics_val.csv
        â”œâ”€â”€ high_school_physics_val.csv
        â”œâ”€â”€ high_school_psychology_val.csv
        â”œâ”€â”€ high_school_statistics_val.csv
        â”œâ”€â”€ high_school_us_history_val.csv
        â”œâ”€â”€ high_school_world_history_val.csv
        â”œâ”€â”€ human_aging_val.csv
        â”œâ”€â”€ human_sexuality_val.csv
        â”œâ”€â”€ international_law_val.csv
        â”œâ”€â”€ jurisprudence_val.csv
        â”œâ”€â”€ logical_fallacies_val.csv
        â”œâ”€â”€ machine_learning_val.csv
        â”œâ”€â”€ management_val.csv
        â”œâ”€â”€ marketing_val.csv
        â”œâ”€â”€ medical_genetics_val.csv
        â”œâ”€â”€ miscellaneous_val.csv
        â”œâ”€â”€ moral_disputes_val.csv
        â”œâ”€â”€ moral_scenarios_val.csv
        â”œâ”€â”€ nutrition_val.csv
        â”œâ”€â”€ philosophy_val.csv
        â”œâ”€â”€ prehistory_val.csv
        â”œâ”€â”€ professional_accounting_val.csv
        â”œâ”€â”€ professional_law_val.csv
        â”œâ”€â”€ professional_medicine_val.csv
        â”œâ”€â”€ professional_psychology_val.csv
        â”œâ”€â”€ public_relations_val.csv
        â”œâ”€â”€ security_studies_val.csv
        â”œâ”€â”€ sociology_val.csv
        â”œâ”€â”€ us_foreign_policy_val.csv
        â”œâ”€â”€ virology_val.csv
        â””â”€â”€ world_religions_val.csv
    ```

## Available Dataset Tasks
### mmlu_gen_5_shot_str
#### Basic Information
| Task Name | Introduction | Evaluation Metric | Few-Shot | Prompt Format | Corresponding Source Code Configuration File Path |
| --- | --- | --- | --- | --- | --- |
| mmlu_gen | Generative task for the MMLU dataset | Accuracy (naive_average) | 5-shot | String format | [mmlu_gen.py](mmlu_gen_5_shot_str.py) |
| mmlu_gen | Generative task for the MMLU dataset, with a logical chain in the prompt (aligned with DeepSeek R1 accuracy test) | Accuracy (naive_average) | 0-shot | String format | [mmlu_gen.py](mmlu_gen_0_shot_cot_chat_prompt.py) |
| mmlu_ppl | MMLU dataset PPL task | Accuracy (naive_average) | 0-shot | String format | [mmlu_ppl_0_shot_str.py](mmlu_ppl_0_shot_str.py) |