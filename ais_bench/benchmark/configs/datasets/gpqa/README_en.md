# GPQA
[ä¸­æ–‡](README.md) | English
## Dataset Introduction
GPQA is a question-answering dataset consisting of multiple-choice questions. The high-difficulty questions within it are written and verified by experts in the fields of biology, physics, and chemistry. When these experts attempt to answer questions outside their own professional domains (for example, a physicist answering chemistry questions), their answer accuracy is only 34%â€”even if they can use Google Search without restrictions and spend more than 30 minutes on each question.

> ğŸ”— Dataset Homepage: [https://github.com/idavidrein/gpqa](https://github.com/idavidrein/gpqa)

## Dataset Deployment
- The dataset compressed package can be downloaded from the link provided by OpenCompass ğŸ”—: [http://opencompass.oss-cn-shanghai.aliyuncs.com/datasets/data/gpqa.zip](http://opencompass.oss-cn-shanghai.aliyuncs.com/datasets/data/gpqa.zip).
- It is recommended to deploy the dataset in the directory `{tool_root_path}/ais_bench/datasets` (the default path set in dataset tasks). Taking deployment on Linux as an example, the specific execution steps are as follows:
```bash
# Within the Linux server, under the tool root path
cd ais_bench/datasets
wget http://opencompass.oss-cn-shanghai.aliyuncs.com/datasets/data/gpqa.zip
unzip gpqa.zip
rm gpqa.zip
```
- Execute `tree gpqa/` in the directory `{tool_root_path}/ais_bench/datasets` to check the directory structure. If the directory structure is as shown below, the dataset has been deployed successfully:
    ```
    gpqa
    â”œâ”€â”€ gpqa_diamond.csv
    â”œâ”€â”€ gpqa_experts.csv
    â”œâ”€â”€ gpqa_extended.csv
    â”œâ”€â”€ gpqa_main.csv
    â””â”€â”€ license.txt
    ```

## Available Dataset Tasks
| Task Name | Introduction | Evaluation Metric | Few-Shot | Prompt Format | Corresponding Source Code Configuration File Path |
| --- | --- | --- | --- | --- | --- |
| gpqa_gen_0_shot_str | Generative task for the GPQA dataset | Accuracy (pass@1) | 0-shot | String format | [gpqa_gen_0_shot_str.py](gpqa_gen_0_shot_str.py) |
| gpqa_gen_0_shot_cot_chat_prompt | Generative task for the GPQA dataset (aligned with DeepSeek R1 accuracy test) | Accuracy (pass@1) | 0-shot | Chat format | [gpqa_gen_0_shot_cot_chat_prompt.py](gpqa_gen_0_shot_cot_chat_prompt.py) |
| gpqa_ppl_0_shot_str | PPL task for the GPQA dataset | Accuracy (pass@1) | 0-shot | String format | [gpqa_ppl_0_shot_str.py](gpqa_ppl_0_shot_str.py) |

### Translation Notes
1. **Term Consistency**: Technical terms such as "ç”Ÿæˆå¼ä»»åŠ¡" (generative task), "è¯„ä¼°æŒ‡æ ‡" (evaluation metric), and "å¯¹é½DeepSeek R1ç²¾åº¦æµ‹è¯•" (aligned with DeepSeek R1 accuracy test) follow standard expressions in AI dataset documentation to ensure clarity for technical users.
2. **Proper Nouns & Acronyms**: "GPQA" (the dataset name) is retained as is; "OpenCompass" (platform name) and "DeepSeek R1" (model/test standard name) remain unchanged to maintain recognition in the technical community.
3. **Code & Path Preservation**: Linux commands (e.g., `cd`, `wget`), directory paths (e.g., `{tool_root_path}/ais_bench/datasets`), and filenames (e.g., `gpqa_diamond.csv`, `gpqa_gen_0_shot_str.py`) are copied exactly to avoid disrupting deployment workflows.
4. **Contextual Accuracy**: The description of expertsâ€™ limited cross-domain performance ("å³ä¾¿ä»–ä»¬èƒ½æ— é™åˆ¶åœ°ä½¿ç”¨è°·æ­Œæœç´¢ï¼Œä¸”èŠ±è´¹è¶…è¿‡ 30 åˆ†é’Ÿä½œç­”ï¼Œç­”é¢˜å‡†ç¡®ç‡ä¹Ÿä»…æœ‰ 34%") is translated to preserve logical relationships (using "even if" to convey the contrast) and key details (time limit, tool access, accuracy rate), ensuring the datasetâ€™s difficulty characteristics are accurately communicated.