<div align="center">
  <br />
  <br />

  # **AISBench Benchmark Tool**
  #### A Testing Benchmark Tool for the Artificial Intelligence Field
  <!-- Use a separator line instead of a background -->
  ---

[![][github-release-shield]][github-release-link]
[![][github-releasedate-shield]][github-releasedate-link]
[![][github-contributors-shield]][github-contributors-link]<br>
[![][github-forks-shield]][github-forks-link]
[![][github-stars-shield]][github-stars-link]
[![][github-issues-shield]][github-issues-link]
[![License](https://img.shields.io/badge/license-Apache--2.0-red?logo=apache)](https://www.apache.org/licenses/LICENSE-2.0)

<br><br>
[üåê Official Website](https://www.aisbench.com) |
[üìñ Tool Documentation](https://ais-bench-benchmark-rf.readthedocs.io/en/latest/) |
[üî• Latest Updates](#-latest-updates)|
[ü§î Report Issues](https://github.com/AISBench/benchmark/issues/new/choose)
<br><br>[ÁÆÄ‰Ωì‰∏≠Êñá](README.md) | English
</div>

> ‚ùó<span style="color: red;"><b>Important</b></span>
>
> **‚≠êÔ∏èStar this project** to get the latest updates of AISBench Benchmark Tool in real time!

## üî• Latest Updates
- **\[2025.11.25\]** Support for PPL (Perplexity-based) mode accuracy evaluation for service-deployed models.üî•üî•üî•
- **\[2025.9.08\]** Support for üìö[Simulating Real Business Traffic](https://ais-bench-benchmark-rf.readthedocs.io/en/latest/advanced_tutorials/rps_distribution.html): By controlling fluctuations in request sending rates, perceive the performance evaluation results of service deployment in simulated real-world scenarios! üî•üî•üî•

- **\[2025.8.28\]** Support for üìö[Multiple Independent Repeated Inference Accuracy Scenarios](https://ais-bench-benchmark-rf.readthedocs.io/en/latest/base_tutorials/scenes_intro/accuracy_benchmark.html#id12), calculating accuracy metrics across different dimensions such as pass@k/cons@k/avg@n! üî¨üî¨üî¨

- **\[2025.8.19\]**
  - Added a dedicated model configuration for Function Call: [vllm_api_function_call_chat](ais_bench/benchmark/configs/models/vllm_api/vllm_api_function_call_chat.py), supporting [BFCL Function Calling Capability Evaluation](ais_bench/benchmark/configs/datasets/BFCL/README_en.md) üî•üî•üî•
  - Provided [Performance Test Specification Documentation](https://ais-bench-benchmark-rf.readthedocs.io/en/latest/base_tutorials/scenes_intro/performance_benchmark.html#id25) supported by the tool, optimizing memory usage and performance calculation of the tool in inference cluster scenarios. For the maximum specification scenario (250K requests, input/output tokens: 4K/4K), memory usage is reduced by 60% (now less than 64GB), and performance result calculation efficiency is improved by 20x. üöÄüöÄüöÄ

- **\[2025.7.15\]**
  - Supported service deployment performance evaluation and visualization for multi-turn dialogue datasets such as [sharegpt](ais_bench/benchmark/configs/datasets/sharegpt/README_en.md) and [mtbench](ais_bench/benchmark/configs/datasets/mtbench/README_en.md). See üìö[Multi-Turn Dialogue Evaluation Guide](https://ais-bench-benchmark-rf.readthedocs.io/en/latest/advanced_tutorials/multiturn_benchmark.html) for evaluation methods! üî•üî•üî•
  - Enabled the use of [custom datasets](https://ais-bench-benchmark-rf.readthedocs.io/en/latest/advanced_tutorials/custom_dataset.html) in performance evaluation scenarios, supporting the specification of maximum output length at the request granularity! üî•üî•üî•

- **\[2025.6.19\]** Support for üìö[Performance Evaluation Result Visualization](https://ais-bench-benchmark-rf.readthedocs.io/en/latest/base_tutorials/results_intro/performance_visualization.html) to help locate performance bottlenecks of inference services! üî•üî•üî•

- **\[2025.6.12\]** Supported accuracy and performance evaluation for multimodal datasets including [textvqa](ais_bench/benchmark/configs/datasets/textvqa/README_en.md), [videobench](ais_bench/benchmark/configs/datasets/videobench/README_en.md), and [vocalsound](ais_bench/benchmark/configs/datasets/vocalsound/README_en.md)! üî•üî•üî•

- **\[2025.6.6\]** AISBench supports steady-state performance evaluation to obtain the true optimal performance of the system. Refer to üìö [Service Deployment Steady-State Performance Test](doc/users_guide/stable_stage.md) to get started quickly! üî•üî•üî•

- **\[2025.5.16\]** Supported performance evaluation for high concurrency service deployment (up to 30,000+ concurrent requests). üìö [Performance Metrics](doc/users_guide/performance_metric.md) are aligned with üîó [vllm benchmark](https://github.com/vllm-project/vllm/tree/main/benchmarks). See üìö [Service Deployment Performance Evaluation Guide](https://ais-bench-benchmark-rf.readthedocs.io/en/latest/base_tutorials/scenes_intro/performance_benchmark.html) for details! üî•üî•üî•

- **\[2025.4.30\]** Accuracy evaluation supports resuming from breakpoints and re-evaluating failed cases, significantly improving the robustness of accuracy evaluation. Refer to üìö [Resume from Interruption & Re-evaluate Failed Cases](https://ais-bench-benchmark-rf.readthedocs.io/en/latest/base_tutorials/scenes_intro/accuracy_benchmark.html#id10) to get started quickly! üî•üî•üî•

- **\[2025.4.15\]** Optimized the request sending method from fixed-batch to continuous batch mode, significantly improving accuracy evaluation efficiency! üî•üî•üî•

- **\[2025.4.12\]** Supported merging all multi-file datasets (such as MMLU, Ceval) into a single dataset task for accuracy evaluation. See üìö [Merge Multi-File Datasets](https://ais-bench-benchmark-rf.readthedocs.io/en/latest/base_tutorials/scenes_intro/accuracy_benchmark.html#id11) for details! üî•üî•üî•


## üåè Introduction
AISBench Benchmark is a model evaluation tool built based on [OpenCompass](https://github.com/open-compass/opencompass). It is compatible with OpenCompass‚Äôs configuration system, dataset structure, and model backend implementation, and on this basis, extends support for service-deployed models.

Currently, AISBench supports evaluation scenarios for two major types of inference tasks:

üîç [Accuracy Evaluation](https://ais-bench-benchmark-rf.readthedocs.io/en/latest/base_tutorials/scenes_intro/home.html#id2): Supports accuracy verification of service-deployed models and local models on various question-answering and reasoning benchmark datasets.

üöÄ [Performance Evaluation](https://ais-bench-benchmark-rf.readthedocs.io/en/latest/base_tutorials/scenes_intro/home.html#id5): Supports latency and throughput evaluation of service-deployed models, as well as extreme performance testing under stress test scenarios.


## üõ†Ô∏è Tool Installation
‚úÖ Environment Requirements

**Python Version**: Only Python **3.10** or **3.11** is supported.

Python 3.9 and below are not supported, nor are versions 3.12 and above.

**It is recommended to use Conda for environment management** to avoid dependency conflicts:
```shell
conda create --name ais_bench python=3.10 -y
conda activate ais_bench
```

üì¶ Installation Method (Source Code Installation)

AISBench currently only provides source code installation. Ensure the installation environment has internet access:
```shell
git clone https://github.com/AISBench/benchmark.git
cd benchmark/
pip3 install -e ./ --use-pep517
```
This command will automatically install core dependencies.
Execute `ais_bench -h`. If the help information for all command-line options of the AISBench evaluation tool is printed, the installation is successful.

‚öôÔ∏è Service Deployment Framework Support (Optional)

If you need to evaluate service-deployed models (such as vLLM, Triton, etc.), install additional dependencies:
```shell
pip3 install -r requirements/api.txt
pip3 install -r requirements/extra.txt
```

üîó Berkeley Function Calling Leaderboard (BFCL) Evaluation Support
```shell
pip3 install -r requirements/datasets/bfcl_dependencies.txt --no-deps
```

**Important Note**: Since `bfcl_eval` automatically installs the `pathlib` library (which is already built into Python 3.5+ environments), use the `--no-deps` parameter to skip automatic installation of additional dependencies to avoid version conflicts.

For further configuration or to initiate evaluation tasks using CLI or Python scripts, refer to the [Quick Start Guide](#quick-start).


## ‚ùå Tool Uninstallation
To uninstall AISBench Benchmark, execute the following command:
```shell
pip3 uninstall ais_bench_benchmark
```


## üöÄ Quick Start
### Command Meaning
A single or multiple evaluation tasks executed by an AISBench command are defined by a combination of model tasks (single or multiple), dataset tasks (single or multiple), and result presentation tasks (single). Other command-line options of AISBench specify the scenario of the evaluation task (accuracy evaluation scenario, performance evaluation scenario, etc.). Take the following AISBench command as an example:
```shell
ais_bench --models vllm_api_general_chat --datasets demo_gsm8k_gen_4_shot_cot_chat_prompt --summarizer example
```
This command does not specify other command-line options, so it defaults to an accuracy evaluation task, where:
- `--models` specifies the model task: the `vllm_api_general_chat` model task.
- `--datasets` specifies the dataset task: the `demo_gsm8k_gen_4_shot_cot_chat_prompt` dataset task.
- `--summarizer` specifies the result presentation task: the `example` result presentation task (if `--summarizer` is not specified, the `example` task is used by default for accuracy evaluation scenarios). It is generally used as default and does not need to be specified in the command line (subsequent commands will omit this option).


### Task Meaning Query (Optional)
Detailed information (introduction, usage constraints, etc.) about the selected model task (`vllm_api_general_chat`), dataset task (`demo_gsm8k_gen_4_shot_cot_chat_prompt`), and result presentation task (`example`) can be queried from the following links:
- `--models`: üìö [Service Deployment Inference Backend](https://ais-bench-benchmark-rf.readthedocs.io/en/latest/base_tutorials/all_params/models.html#id2)
- `--datasets`: üìö [Open-Source Datasets](https://ais-bench-benchmark-rf.readthedocs.io/en/latest/base_tutorials/all_params/datasets.html#id3) ‚Üí üìö [Detailed Introduction](ais_bench/benchmark/configs/datasets/demo/README_en.md)
- `--summarizer`: üìö [Result Summary Tasks](https://ais-bench-benchmark-rf.readthedocs.io/en/latest/base_tutorials/all_params/summarizer.html)


# Modification of Configuration Files Corresponding to Tasks
Each model task, dataset task, and result presentation task corresponds to a configuration file. The content of these configuration files needs to be modified before running the command. The paths of these configuration files can be queried by adding `--search` to the original AISBench command. For example:
```shell
ais_bench --models vllm_api_general_chat --datasets demo_gsm8k_gen_4_shot_cot_chat_prompt --search
```
> ‚ö†Ô∏è **Note**: Executing the command with the `search` option will print the absolute path of the configuration file corresponding to the task.

After executing the query command, you will get the following query results:
```shell
‚ïí‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï§‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï§‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïï
‚îÇ Task Type    ‚îÇ Task Name                             ‚îÇ Config File Path                                                                                                               ‚îÇ
‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°
‚îÇ --models     ‚îÇ vllm_api_general_chat                 ‚îÇ /your_workspace/benchmark/ais_bench/benchmark/configs/models/vllm_api/vllm_api_general_chat.py                                 ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ --datasets   ‚îÇ demo_gsm8k_gen_4_shot_cot_chat_prompt ‚îÇ /your_workspace/benchmark/ais_bench/benchmark/configs/datasets/demo/demo_gsm8k_gen_4_shot_cot_chat_prompt.py                   ‚îÇ
‚ïò‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïß‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïß‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïõ

```

- The dataset task configuration file `demo_gsm8k_gen_4_shot_cot_chat_prompt.py` in the quick start does not require additional modifications. For an introduction to the content of the dataset task configuration file, please refer to üìö [Configure Open-Source Datasets](https://ais-bench-benchmark-rf.readthedocs.io/en/latest/base_tutorials/all_params/datasets.html#id6)

The model configuration file `vllm_api_general_chat.py` contains configuration content related to model operation and needs to be modified according to actual conditions. The content that needs to be modified in the quick start is marked with comments.
```python
from ais_bench.benchmark.models import VLLMCustomAPIChat

models = [
    dict(
        attr="service",
        type=VLLMCustomAPIChat,
        abbr='vllm-api-general-chat',
        path="",                    # Specify the absolute path of the model serialized vocabulary file (generally not required for accuracy testing scenarios)
        model="",        # Specify the name of the model loaded on the server, configured according to the actual model name pulled by the VLLM inference service (configure as an empty string to get it automatically)
        stream=False,
        request_rate=0,           # Request sending frequency: send 1 request to the server every 1/request_rate seconds; if less than 0.1, all requests are sent at once
        retry=2,                  # Maximum number of retries for each request
        api_key="",               # Custom API key, default is an empty string
        host_ip="localhost",      # Specify the IP of the inference service
        host_port=8080,           # Specify the port of the inference service
        url="",                     # Custom URL path for accessing the inference service (needs to be configured when the base URL is not a combination of http://host_ip:host_port; after configuration, host_ip and host_port will be ignored)
        max_out_len=512,          # Maximum number of tokens output by the inference service
        batch_size=1,               # Maximum concurrency for sending requests
        trust_remote_code=False,    # Whether the tokenizer trusts remote code, default is False;
        generation_kwargs=dict(   # Model inference parameters, configured with reference to the VLLM documentation; the AISBench evaluation tool does not process them and attaches them to the sent request
            temperature=0.01,
            ignore_eos=False,
        )
    )
]
```
# Execution Command
After modifying the configuration files, execute the command to start the service-based accuracy evaluation:
```bash
ais_bench --models vllm_api_general_chat --datasets demo_gsm8k_gen_4_shot_cot_chat_prompt
```
## View Task Execution Details
After executing the AISBench command, the status of the ongoing task will be displayed on a real-time refreshed dashboard in the command line (press the "P" key on the keyboard to stop refreshing for copying dashboard information, and press "P" again to resume refreshing). For example:
```
Base path of result&log : outputs/default/20250628_151326
Task Progress Table (Updated at: 2025-11-06 10:08:21)
Page: 1/1  Total 2 rows of data
Press Up/Down arrow to page,  'P' to PAUZE/RESUME screen refresh, 'Ctrl + C' to exit

+----------------------------------+-----------+-------------------------------------------------+-------------+-------------+-------------------------------------------------+------------------------------------------------+
| Task Name                        |   Process | Progress                                        | Time Cost   | Status      | Log Path                                        | Extend Parameters                              |
+==================================+===========+=================================================+=============+=============+=================================================+================================================+
| vllm-api-general-chat/demo_gsm8k |    547141 | [###############               ] 4/8 [0.5 it/s] | 0:00:11     | inferencing | logs/infer/vllm-api-general-chat/demo_gsm8k.out | {'POST': 5, 'RECV': 4, 'FINISH': 4, 'FAIL': 0} |
+----------------------------------+-----------+-------------------------------------------------+-------------+-------------+-------------------------------------------------+------------------------------------------------+

```

The detailed logs of task execution will be continuously saved to the default output path, which is displayed on the real-time refreshed dashboard as `Log Path`. The `Log Path` (`logs/infer/vllm-api-general-chat/demo_gsm8k.out`) is under the `Base path` (`outputs/default/20250628_151326`). Taking the above dashboard information as an example, the path of the detailed log of task execution is:
```shell
# {Base path}/{Log Path}
outputs/default/20250628_151326/logs/infer/vllm-api-general-chat/demo_gsm8k.out
```

> üí° If you want the detailed logs to be printed directly during execution, you can add `--debug` to the execution command:
`ais_bench --models vllm_api_general_chat --datasets demo_gsm8k_gen_4_shot_cot_chat_prompt --debug`


The `Base path` (`outputs/default/20250628_151326`) contains all the task execution details. After the command execution is completed, all execution details are as follows:
```shell
20250628_151326/
‚îú‚îÄ‚îÄ configs # A combined configuration of the configuration files corresponding to model tasks, dataset tasks, and structure presentation tasks
‚îÇ   ‚îî‚îÄ‚îÄ 20250628_151326_29317.py
‚îú‚îÄ‚îÄ logs # Logs during execution; if --debug is added to the command, no process logs will be saved (all logs are printed directly)
‚îÇ   ‚îú‚îÄ‚îÄ eval
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ vllm-api-general-chat
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ demo_gsm8k.out # Logs of the accuracy evaluation process based on the inference results in the predictions/ folder
‚îÇ   ‚îî‚îÄ‚îÄ infer
‚îÇ       ‚îî‚îÄ‚îÄ vllm-api-general-chat
‚îÇ           ‚îî‚îÄ‚îÄ demo_gsm8k.out # Logs of the inference process
‚îú‚îÄ‚îÄ predictions
‚îÇ   ‚îî‚îÄ‚îÄ vllm-api-general-chat
‚îÇ       ‚îî‚îÄ‚îÄ demo_gsm8k.json # Inference results (all outputs returned by the inference service)
‚îú‚îÄ‚îÄ results
‚îÇ   ‚îî‚îÄ‚îÄ vllm-api-general-chat
‚îÇ       ‚îî‚îÄ‚îÄ demo_gsm8k.json # Original scores calculated by accuracy evaluation
‚îî‚îÄ‚îÄ summary
    ‚îú‚îÄ‚îÄ summary_20250628_151326.csv # Final accuracy scores (in table format)
    ‚îú‚îÄ‚îÄ summary_20250628_151326.md # Final accuracy scores (in Markdown format)
    ‚îî‚îÄ‚îÄ summary_20250628_151326.txt # Final accuracy scores (in text format)
```
> ‚ö†Ô∏è **Note**: The content of the saved task execution details varies in different evaluation scenarios. For details, please refer to the guide for the specific evaluation scenario.


#### Output Results
Since there are only 8 data entries, results will be generated quickly. An example of the output is shown below:
```bash
dataset                 version  metric   mode  vllm_api_general_chat
----------------------- -------- -------- ----- ----------------------
demo_gsm8k              401e4c   accuracy gen                   62.50
```

For more tutorials, please refer to our üëâ[Documentation](https://ais-bench-benchmark-rf.readthedocs.io/en/latest/)


## üîú Coming Soon
- [ ] **\[2025.10\]** Complete a full refactoring of AISBench to support plug-and-play integration of cutting-edge testing benchmarks within the AISBench framework, addressing the increasingly complex and diverse testing tasks in the industry; while significantly improving usability.
- [ ] **\[2025.11\]** Provide industry-leading multimodal evaluation capabilities.
- [ ] **\[2025.12\]** Provide evaluation capabilities for mainstream industry Agents.
- [x] **\[2025.9\]** Support simulating real task traffic.
- [x] **\[2025.8\]** Add support for performance evaluation of multi-turn dialogue datasets such as ShareGPT and BFCL.
- [x] **\[2025.8\]** Optimize the calculation efficiency of the eval phase in performance evaluation, reduce the tool‚Äôs memory usage, and supplement specifications for tool usage.
- [x] **\[2025.7\]** Enable the use of custom datasets in performance evaluation scenarios, supporting the definition of maximum output length limits for individual data entries.


## ü§ù Acknowledgements
- The code of this project is developed based on üîó [OpenCompass](https://github.com/open-compass/opencompass) with extensions.
- Some datasets and prompt implementations in this project are modified from [simple-evals](https://github.com/openai/simple-evals).
- The performance metrics tracked in this project‚Äôs code are aligned with [VLLM Benchmark](https://github.com/vllm-project/vllm/tree/main/benchmarks).
- The BFCL function calling capability evaluation feature of this project is implemented based on the [Berkeley Function Calling Leaderboard (BFCL)](https://github.com/ShishirPatil/gorilla/tree/main/berkeley-function-call-leaderboard).


<p align="right"><a href="#top">üîùBack to top</a></p>

[github-contributors-link]: https://github.com/AISBench/benchmark/graphs/contributors
[github-contributors-shield]: https://img.shields.io/github/contributors/AISBench/benchmark?color=c4f042&labelColor=black&style=flat-square
[github-forks-link]: https://github.com/AISBench/benchmark/network/members
[github-forks-shield]: https://img.shields.io/github/forks/AISBench/benchmark?color=8ae8ff&labelColor=black&style=flat-square
[github-issues-link]: https://github.com/AISBench/benchmark/issues
[github-issues-shield]: https://img.shields.io/github/issues/AISBench/benchmark?color=ff80eb&labelColor=black&style=flat-square
[github-license-link]: https://github.com/AISBench/benchmark/blob/main/LICENSE
[github-license-shield]: https://img.shields.io/github/license/AISBench/benchmark?color=white&labelColor=black&style=flat-square
[github-release-link]: https://github.com/AISBench/benchmark/releases
[github-release-shield]:  https://img.shields.io/github/v/release/AISBench/benchmark?color=369eff&labelColor=black&logo=github&style=flat-square
[github-releasedate-link]: https://github.com/AISBench/benchmark/releases
[github-releasedate-shield]: https://img.shields.io/github/release-date/AISBench/benchmark?labelColor=black&style=flat-square
[github-stars-link]: https://github.com/AISBench/benchmark/stargazers
[github-stars-shield]: https://img.shields.io/github/stars/AISBench/benchmark?color=ffcb47&labelColor=black&style=flat-square
[github-trending-shield]: https://trendshift.io/api/badge/repositories/6630
[github-trending-url]: https://trendshift.io/repositories/6630